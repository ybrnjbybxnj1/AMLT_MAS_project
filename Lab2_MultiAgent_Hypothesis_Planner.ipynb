{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json, re\n",
    "from datetime import datetime\n",
    "from typing import List, Optional, Literal, TypedDict, Annotated\n",
    "import operator\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from pydantic import BaseModel, Field, ConfigDict\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langgraph.graph import StateGraph, END\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(find_dotenv(usecwd=True))\n",
    "BASE_URL = os.getenv(\"LITELLM_BASE_URL\", add your base url)\n",
    "API_KEY = os.getenv(\"LITELLM_API_KEY\", add your api key)\n",
    "MODEL_NAME = os.getenv(\"MODEL_NAME\", \"qwen3-32b\")\n",
    "llm = ChatOpenAI(base_url=BASE_URL, api_key=API_KEY, model=MODEL_NAME, temperature=0.7, max_tokens=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pydantic Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Paper(BaseModel):\n",
    "    title: str; abstract: str = \"\"; \n",
    "    year: Optional[int] = None; \n",
    "    source: str = \"arxiv\"; \n",
    "    url: Optional[str] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryClassification(BaseModel):\n",
    "    query_type: Literal[\"conceptual\", \"design\", \"implementation\", \"planning\"]\n",
    "    confidence: Literal[\"high\", \"medium\", \"low\"]; \n",
    "    reasoning: str\n",
    "    needs_memory: bool; \n",
    "    is_followup: bool; \n",
    "    target_agents: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrendAnalysis(BaseModel):\n",
    "    trends: List[str]; \n",
    "    emerging_directions: List[str]; \n",
    "    confidence: Literal[\"high\", \"medium\", \"low\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContradictionAnalysis(BaseModel):\n",
    "    contradictions: List[str]; \n",
    "    unsolved_problems: List[str]; \n",
    "    opportunities: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hypothesis(BaseModel):\n",
    "    statement: str; \n",
    "    triz_principles: List[str]; \n",
    "    rationale: str; novelty_score: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentPlan(BaseModel):\n",
    "    feasibility: Literal[\"high\", \"medium\", \"low\"]; \n",
    "    steps: List[str]\n",
    "    resources: List[str]; \n",
    "    duration: str; \n",
    "    challenges: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryEntry(BaseModel): \n",
    "    query: str; \n",
    "    response_summary: str; \n",
    "    agents_used: List[str]; \n",
    "    key_findings: List[str] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_lists(l, r): \n",
    "    return (l or []) + (r or [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    user_query: str; \n",
    "    classification: Optional[QueryClassification]; \n",
    "    current_agent: Optional[str]\n",
    "    agents_activated: Annotated[List[str], operator.add]; \n",
    "    literature_data: Optional[dict]\n",
    "    papers: Annotated[List, merge_lists]; \n",
    "    trends: Optional[TrendAnalysis]; \n",
    "    gaps: Optional[ContradictionAnalysis]\n",
    "    hypothesis: Optional[Hypothesis]; \n",
    "    novelty_score: Optional[dict]; \n",
    "    experiment_plan: Optional[ExperimentPlan]\n",
    "    feasibility_score: Optional[dict]; \n",
    "    final_response: Optional[str]; \n",
    "    messages: Annotated[List[str], operator.add]\n",
    "    session_history: Annotated[List, merge_lists]; \n",
    "    notes: Annotated[List[str], operator.add]; \n",
    "    memory_context: Optional[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_initial_state(q): \n",
    "    return {k: [] if 'List' in str(v) else None for k,v in AgentState.__annotations__.items()} | {'user_query': q}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if not text: return \"\"\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('ascii')\n",
    "    return ' '.join(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_arxiv(query, max_results=10):\n",
    "    print(f\"[TOOL] ArXiv: {query}\")\n",
    "    try:\n",
    "        r = requests.get(\"http://export.arxiv.org/api/query\", params={'search_query': f'all:{query}', 'max_results': max_results}, timeout=15)\n",
    "        root = ET.fromstring(r.content)\n",
    "        papers = []\n",
    "        for e in root.findall('{http://www.w3.org/2005/Atom}entry'):\n",
    "            t = e.find('{http://www.w3.org/2005/Atom}title')\n",
    "            s = e.find('{http://www.w3.org/2005/Atom}summary')\n",
    "            if t is not None and s is not None:\n",
    "                clean_title = clean_text(t.text)\n",
    "                clean_abstract = clean_text(s.text)\n",
    "                papers.append(Paper(title=clean_title, abstract=clean_abstract))\n",
    "        print(f\"[TOOL] ArXiv found {len(papers)} papers\")\n",
    "        return {\"papers_found\": len(papers), \"key_topics\": [p.title for p in papers[:5]], \"papers\": papers}\n",
    "    except Exception as e: \n",
    "        return {\"papers_found\": 0, \"papers\": [], \"error\": str(e)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_novelty(hyp, papers):\n",
    "    if not papers: return {\"score\": 7, \"reason\": \"No papers\"}\n",
    "    hw = set(re.findall(r'\\b\\w{4,}\\b', hyp.lower()))\n",
    "    overlaps = [len(hw & set(re.findall(r'\\b\\w{4,}\\b', f\"{p.title} {p.abstract}\".lower())))/len(hw) for p in papers[:10] if hw]\n",
    "    return {\"score\": max(1,min(10,int((1-np.mean(overlaps))*10))) if overlaps else 7, \"reason\": f\"Overlap: {np.mean(overlaps)*100:.1f}%\" if overlaps else \"N/A\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_feasibility(exp):\n",
    "    s = 10 - (2 if any(x in ' '.join(exp.resources).lower() for x in ['supercomputer','quantum','a100']) else 0) - (2 if 'year' in exp.duration.lower() else 0)\n",
    "    return {\"category\": \"high\" if s>=7 else \"medium\" if s>=4 else \"low\", \"score\": max(1,s)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRIZ = [\"Segmentation\",\"Taking out\",\"Local quality\",\"Asymmetry\",\"Merging\",\"Universality\",\"Dynamics\",\"Feedback\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory and LLM utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_retry(): # retry decorator for LLM calls\n",
    "    return retry(\n",
    "        stop=stop_after_attempt(3),\n",
    "        wait=wait_exponential(multiplier=1, min=2, max=10),\n",
    "        retry=retry_if_exception_type((Exception,)),\n",
    "        reraise=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@llm_retry() \n",
    "def invoke_with_parser(llm, parser, prompt_template, **kwargs): \n",
    "    if kwargs:\n",
    "        try:\n",
    "            prompt = prompt_template.format(**kwargs)\n",
    "        except KeyError as e:\n",
    "            print(f\"[LLM] Warning: Prompt formatting failed ({e}), using raw string.\")\n",
    "            prompt = prompt_template\n",
    "    else:\n",
    "        prompt = prompt_template\n",
    "    response = llm.invoke([\n",
    "        SystemMessage(content=f\"Respond with valid JSON matching this schema:\\n{parser.get_format_instructions()}\"),\n",
    "        HumanMessage(content=prompt)\n",
    "    ])\n",
    "    content = response.content.strip()\n",
    "    content = re.sub(r'<think>.*?</think>', '', content, flags=re.DOTALL).strip()\n",
    "    if content.startswith(\"```\"):\n",
    "        content = re.sub(r'^```\\w*\\n?|\\n?```', '', content)\n",
    "    json_match = re.search(r'(\\{[\\s\\S]*\\}|\\[[\\s\\S]*\\])', content)\n",
    "    if json_match:\n",
    "        content = json_match.group(1)\n",
    "    content = re.sub(r'\\\\x([0-9a-fA-F]{2})', r'\\\\u00\\1', content) \n",
    "    content = content.replace(r'\\x2014', '-')     \n",
    "    return parser.parse(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemStore: \n",
    "    def __init__(self, filepath=\"memory.json\"):\n",
    "        self.filepath = filepath\n",
    "        self.history = self._load() \n",
    "        self.notes = []\n",
    "    def _load(self):\n",
    "        if os.path.exists(self.filepath):\n",
    "            try:\n",
    "                with open(self.filepath, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                    return [MemoryEntry(**d) for d in data]\n",
    "            except Exception as e:\n",
    "                print(f\"[MEMORY] error loading file: {e}\")\n",
    "        return []\n",
    "    def _save(self):\n",
    "        try:\n",
    "            data = [entry.dict() for entry in self.history]\n",
    "            with open(self.filepath, 'w') as f:\n",
    "                json.dump(data, f, indent=2)\n",
    "        except Exception as e:\n",
    "            print(f\"[MEMORY] error saving file: {e}\")\n",
    "    def add(self, q, r, a, f=None):\n",
    "        entry = MemoryEntry(\n",
    "            query=q,\n",
    "            response_summary=r,\n",
    "            agents_used=a,\n",
    "            key_findings=f or []\n",
    "        )\n",
    "        self.history.append(entry)\n",
    "        self._save()  \n",
    "    def context(self, q, n=3): \n",
    "        return \"\\n\".join([f\"Q:{e.query[:60]}->R:{e.response_summary[:80]}\" for e in self.history[-n:]]) if self.history else \"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem = MemStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Pydantic parsers for each output type\n",
    "query_parser = PydanticOutputParser(pydantic_object=QueryClassification)\n",
    "trend_parser = PydanticOutputParser(pydantic_object=TrendAnalysis)\n",
    "gap_parser = PydanticOutputParser(pydantic_object=ContradictionAnalysis)\n",
    "hypothesis_parser = PydanticOutputParser(pydantic_object=Hypothesis)\n",
    "experiment_parser = PydanticOutputParser(pydantic_object=ExperimentPlan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent nodes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def router_node(state):\n",
    "    # router agent\n",
    "    q, ctx = state[\"user_query\"], mem.context(state[\"user_query\"])\n",
    "    prompt = f\"\"\"\n",
    "    Classify this research query.\n",
    "\n",
    "    Query: {q}\n",
    "    {f'Previous context: {ctx}' if ctx else 'No previous context.'}\n",
    "    \n",
    "    Query types:\n",
    "    - conceptual: Theory questions, concepts, comparisons\n",
    "    - design: Architecture, hypothesis design, methodology  \n",
    "    - implementation: Code, practical how-to, technical details\n",
    "    - planning: Full research workflow, complete plans\n",
    "    \n",
    "    Determine the query type, confidence, whether memory is needed, if it's a follow-up, and which agents should handle it.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        classification = invoke_with_parser(llm, query_parser, prompt)\n",
    "        print(f\"[ROUTER] {classification.query_type} -> {classification.target_agents}\")\n",
    "        return {\"classification\": classification, \"agents_activated\": [\"router\"], \"messages\": [f\"Router: {classification.query_type}\"]}\n",
    "    except Exception as e:\n",
    "        print(f\"[ROUTER] parse failed after retries: {e}, using fallback\")\n",
    "        fallback = QueryClassification(\n",
    "            query_type=\"planning\", confidence=\"low\", reasoning=\"Fallback due to parse error\",\n",
    "            needs_memory=bool(ctx), is_followup=False,\n",
    "            target_agents=[\"research_analyst\", \"hypothesis_generator\", \"experiment_designer\"]\n",
    "        )\n",
    "        return {\"classification\": fallback, \"agents_activated\": [\"router\"], \"messages\": [\"Router: fallback\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def research_node(state):\n",
    "    # research snalyst with Pydantic parsing and retry\n",
    "    q = state[\"user_query\"]\n",
    "    print(f\"[RESEARCH] {q}...\")\n",
    "    # call ArXiv search \n",
    "    lit = search_arxiv(' '.join(q.split()))\n",
    "    papers = lit.get(\"papers\", [])\n",
    "    pc = \"\\n\".join([f\"- {p.title}: {p.abstract}...\" for p in papers]) if papers else \"No papers found\"\n",
    "    trend_prompt = f\"\"\"\n",
    "    Analyze research trends from these papers.\n",
    "\n",
    "    Papers:\n",
    "    {pc}\n",
    "\n",
    "    Research focus: {q}\n",
    "\n",
    "    Identify current trends, emerging directions, and your confidence level.\"\"\"\n",
    "    try:\n",
    "        trends = invoke_with_parser(llm, trend_parser, trend_prompt)\n",
    "    except Exception as e:\n",
    "        print(f\"[RESEARCH] trend parse failed: {e}\")\n",
    "        trends = TrendAnalysis(trends=[\"Emerging AI research\"], emerging_directions=[\"Novel methodologies\"], confidence=\"medium\")\n",
    "    # gap analysis with retry and Pydantic\n",
    "    gap_prompt = f\"\"\"\n",
    "    Find research gaps and opportunities from these papers.\n",
    "\n",
    "    Papers:\n",
    "    {pc}\n",
    "\n",
    "    Identify contradictions in the literature, unsolved problems, and research opportunities.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        gaps = invoke_with_parser(llm, gap_parser, gap_prompt)\n",
    "    except Exception as e:\n",
    "        print(f\"[RESEARCH] gap parse failed: {e}\")\n",
    "        gaps = ContradictionAnalysis(contradictions=[\"Limited scope\"], unsolved_problems=[\"Scalability\"], opportunities=[\"Novel approaches\"])\n",
    "    print(f\"[RESEARCH] {len(papers)} papers, {len(trends.trends)} trends, {len(gaps.opportunities)} opportunities\")\n",
    "    return {\"literature_data\": lit, \"papers\": papers, \"trends\": trends, \"gaps\": gaps, \n",
    "            \"agents_activated\": [\"research_analyst\"], \"messages\": [f\"Research: {len(papers)} papers\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypothesis_node(state):\n",
    "    # hypothesis generator\n",
    "    q, tr, ga, papers = state[\"user_query\"], state.get(\"trends\"), state.get(\"gaps\"), state.get(\"papers\", [])\n",
    "    print(f\"[HYPOTHESIS] generating...\")\n",
    "    tl = (tr.trends if tr else [\"Emerging research\"])[:3]\n",
    "    gl = (ga.opportunities if ga else [\"Novel approaches\"])[:3]\n",
    "    hyp_prompt = f\"\"\"\n",
    "    Generate a research hypothesis using TRIZ methodology.\n",
    "\n",
    "    Research question: {q}\n",
    "\n",
    "    Current trends: {', '.join(tl)}\n",
    "    Research opportunities: {', '.join(gl)}\n",
    "\n",
    "    TRIZ Principles to consider: {', '.join(TRIZ[:5])}\n",
    "\n",
    "    Create a specific, testable hypothesis with:\n",
    "    - A clear statement (minimum 20 characters)\n",
    "    - Which TRIZ principles apply\n",
    "    - Rationale for why this hypothesis matters\n",
    "    - Novelty score (1-10)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        hypothesis = invoke_with_parser(llm, hypothesis_parser, hyp_prompt)\n",
    "        nv = calc_novelty(hypothesis.statement, papers)\n",
    "        print(f\"[HYPOTHESIS] created, novelty: {nv['score']}/10\")\n",
    "        return {\"hypothesis\": hypothesis, \"novelty_score\": nv, \n",
    "                \"agents_activated\": [\"hypothesis_generator\"], \"messages\": [f\"Hypothesis: novelty {nv['score']}\"]}\n",
    "    except Exception as e:\n",
    "        print(f\"[HYPOTHESIS] parse failed: {e}\")\n",
    "        fallback = Hypothesis(\n",
    "            statement=f\"Applying {TRIZ[0]} principle to {q[:40]} will improve research outcomes\",\n",
    "            triz_principles=TRIZ[:2], rationale=\"Addresses identified research gaps through systematic innovation\",\n",
    "            novelty_score=6\n",
    "        )\n",
    "        return {\"hypothesis\": fallback, \"novelty_score\": {\"score\": 6}, \n",
    "                \"agents_activated\": [\"hypothesis_generator\"], \"messages\": [\"Hypothesis: fallback\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_node(state):\n",
    "    # experiment designer\n",
    "    q, h = state[\"user_query\"], state.get(\"hypothesis\")\n",
    "    print(f\"[EXPERIMENT] designing...\")\n",
    "    hs = h.statement if h else f\"Test the approach: {q}\"\n",
    "    exp_prompt = f\"\"\"\n",
    "    Design an experiment to test this hypothesis.\n",
    "\n",
    "    Hypothesis: {hs}\n",
    "    Research context: {q}\n",
    "\n",
    "    Create a practical experiment plan with:\n",
    "    - Feasibility assessment (high/medium/low)\n",
    "    - 3-7 concrete steps\n",
    "    - Required resources\n",
    "    - Estimated duration\n",
    "    - Potential challenges\n",
    "    \"\"\"\n",
    "    try:\n",
    "        experiment = invoke_with_parser(llm, experiment_parser, exp_prompt)\n",
    "        feas = calc_feasibility(experiment)\n",
    "        print(f\"[EXPERIMENT] {len(experiment.steps)} steps, feasibility: {feas['category']}\")\n",
    "        return {\"experiment_plan\": experiment, \"feasibility_score\": feas,\n",
    "                \"agents_activated\": [\"experiment_designer\"], \"messages\": [f\"Experiment: {len(experiment.steps)} steps\"]}\n",
    "    except Exception as e:\n",
    "        print(f\"[EXPERIMENT] parse failed: {e}\")\n",
    "        fallback = ExperimentPlan(\n",
    "            feasibility=\"medium\",\n",
    "            steps=[\"Define experimental setup\", \"Prepare datasets\", \"Implement approach\", \"Run experiments\", \"Analyze results\"],\n",
    "            resources=[\"Computing resources\", \"Datasets\", \"Evaluation metrics\"],\n",
    "            duration=\"4-6 weeks\",\n",
    "            challenges=[\"Data availability\", \"Computational constraints\"]\n",
    "        )\n",
    "        return {\"experiment_plan\": fallback, \"feasibility_score\": {\"category\": \"medium\", \"score\": 7},\n",
    "                \"agents_activated\": [\"experiment_designer\"], \"messages\": [\"Experiment: fallback\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mem_ret_node(state):\n",
    "    # memory retrieval node\n",
    "    ctx = mem.context(state[\"user_query\"])\n",
    "    print(f\"[MEMORY] retrieved context: {len(ctx)} chars\")\n",
    "    return {\"memory_context\": ctx or None, \"agents_activated\": [\"memory_manager\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mem_upd_node(state):\n",
    "    # memory update node\n",
    "    findings = []\n",
    "    h = state.get(\"hypothesis\")\n",
    "    if h and h.statement:\n",
    "        findings.append(f\"Hypothesis: {h.statement[:80]}\")\n",
    "    tr = state.get(\"trends\")\n",
    "    if tr and tr.trends:\n",
    "        findings.append(f\"Trends: {', '.join(tr.trends[:3])}\")\n",
    "    ga = state.get(\"gaps\")\n",
    "    if ga and ga.opportunities:\n",
    "        findings.append(f\"Opportunities: {', '.join(ga.opportunities[:3])}\")\n",
    "    e = state.get(\"experiment_plan\")\n",
    "    if e and e.steps:\n",
    "        findings.append(f\"Experiment: {len(e.steps)} steps, {e.feasibility} feasibility\")\n",
    "    mem.add(state[\"user_query\"], state.get(\"final_response\", \"\")[:150], \n",
    "            list(set(state.get(\"agents_activated\", []))), findings)\n",
    "    print(f\"[MEMORY] saved interaction with {len(findings)} key findings\")\n",
    "    return {\"agents_activated\": [\"memory_manager\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@llm_retry()\n",
    "def synth_llm_call(llm, parts):\n",
    "    # synthesizer LLM call\n",
    "    return llm.invoke([\n",
    "        SystemMessage(content=\"Synthesize multi-agent findings into a clear, comprehensive response.\"),\n",
    "        HumanMessage(content=\"\\n\".join(parts))\n",
    "    ]).content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synth_node(state):\n",
    "    # synthesizer node with retry\n",
    "    q = state[\"user_query\"]\n",
    "    c = state.get(\"classification\")\n",
    "    tr, ga = state.get(\"trends\"), state.get(\"gaps\")\n",
    "    h, e = state.get(\"hypothesis\"), state.get(\"experiment_plan\")\n",
    "    nv, fv = state.get(\"novelty_score\", {}), state.get(\"feasibility_score\", {})\n",
    "    print(f\"[SYNTH] generating response...\")\n",
    "    parts = [f\"Query: {q}\"]\n",
    "    if c: parts.append(f\"Query type: {c.query_type}\")\n",
    "    if tr: parts.append(f\"Trends: {', '.join(tr.trends)}\")\n",
    "    if ga: parts.append(f\"Opportunities: {', '.join(ga.opportunities)}\")\n",
    "    if h: \n",
    "        parts.append(f\"Hypothesis: {h.statement}\")\n",
    "        parts.append(f\"TRIZ principles: {', '.join(h.triz_principles)}\")\n",
    "        parts.append(f\"Novelty score: {nv.get('score', h.novelty_score)}/10\")\n",
    "    if e:\n",
    "        parts.append(f\"Experiment: {len(e.steps)} steps, Duration: {e.duration}\")\n",
    "        parts.append(f\"Feasibility: {fv.get('category', e.feasibility)}\")\n",
    "    try:\n",
    "        response = synth_llm_call(llm, parts)\n",
    "    except Exception as ex:\n",
    "        print(f\"[SYNTH] LLM failed after retries: {ex}\")\n",
    "        response = \"\\n\".join(parts)\n",
    "    print(f\"[SYNTH] response: {len(response)} chars\")\n",
    "    return {\"final_response\": response, \"agents_activated\": [\"synthesizer\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph construction + run function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent name mapping is needed to map LLM-returned names -> actual node names cuz I encountered some issues with that\n",
    "AGENT_NAME_MAP = {\n",
    "    # research-related\n",
    "    \"research\": \"research_analyst\", \"researcher\": \"research_analyst\", \"research_analyst\": \"research_analyst\",\n",
    "    \"research assistant\": \"research_analyst\", \"researchassistant\": \"research_analyst\",\n",
    "    \"theory agent\": \"research_analyst\", \"theoryagent\": \"research_analyst\",\n",
    "    \"literature\": \"research_analyst\", \"analyst\": \"research_analyst\",\n",
    "    \"researchagent\": \"research_analyst\", \"theoryanalysis\": \"research_analyst\",\n",
    "    \"researchdesign\": \"research_analyst\", \"researchplanningagent\": \"research_analyst\",\n",
    "    \"literaturereview\": \"research_analyst\", \"literatureagent\": \"research_analyst\",\n",
    "    # hypothesis-related\n",
    "    \"hypothesis\": \"hypothesis_generator\", \"hypothesisgenerator\": \"hypothesis_generator\",\n",
    "    \"hypothesis_generator\": \"hypothesis_generator\", \"hypothesisdesignagent\": \"hypothesis_generator\",\n",
    "    \"design\": \"hypothesis_generator\", \"designer\": \"hypothesis_generator\",\n",
    "    \"multiagentsystemsagent\": \"hypothesis_generator\", \"multiagentarchitect\": \"hypothesis_generator\",\n",
    "    \"systemarchitecture\": \"hypothesis_generator\", \"systemdesignagent\": \"hypothesis_generator\",\n",
    "    \"designagents\": \"hypothesis_generator\", \"trizexperts\": \"hypothesis_generator\",\n",
    "    \"trizagent\": \"hypothesis_generator\", \"architectureagent\": \"hypothesis_generator\",\n",
    "    \"multiagentsystemarchitect\": \"hypothesis_generator\",\n",
    "    # experiment-related\n",
    "    \"experiment\": \"experiment_designer\", \"experimentdesigner\": \"experiment_designer\",\n",
    "    \"experiment_designer\": \"experiment_designer\", \"implementation\": \"experiment_designer\",\n",
    "    \"planner\": \"experiment_designer\", \"researchplanner\": \"experiment_designer\",\n",
    "    \"memorysystemdesigner\": \"experiment_designer\", \"memorymanagementagent\": \"experiment_designer\",\n",
    "    \"langgraphdeveloperagent\": \"experiment_designer\", \"errorhandlingspecialist\": \"experiment_designer\",\n",
    "    \"implementationagent\": \"experiment_designer\", \"codeagent\": \"experiment_designer\",\n",
    "    \"developeragent\": \"experiment_designer\", \"practicalagent\": \"experiment_designer\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_agent_name(name):\n",
    "    # maps LLM-returned agent names to actual node names \n",
    "    normalized = name.lower().replace(\" \", \"\").replace(\"_\", \"\").replace(\"-\", \"\")\n",
    "    return AGENT_NAME_MAP.get(normalized, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_agents_normalized(target_agents):\n",
    "    # normalizes a list of target agents to actual node names\n",
    "    normalized = set()\n",
    "    for agent in target_agents:\n",
    "        mapped = normalize_agent_name(agent)\n",
    "        if mapped:\n",
    "            normalized.add(mapped)\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_after_router(state):\n",
    "    c = state.get(\"classification\")\n",
    "    if not c: return \"research_analyst\"\n",
    "    if c.needs_memory: return \"memory_retrieval\"\n",
    "    if c.query_type == \"conceptual\": return \"research_analyst\"\n",
    "    elif c.query_type == \"design\": return \"research_analyst\"\n",
    "    elif c.query_type == \"implementation\": return \"experiment_designer\"\n",
    "    else: return \"research_analyst\"  # planning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_after_memory(state):\n",
    "    # normalizes agent names and fallback to query_type\n",
    "    c = state.get(\"classification\")\n",
    "    if not c: return \"research_analyst\"\n",
    "    normalized_agents = get_target_agents_normalized(c.target_agents)\n",
    "    print(f\"[ROUTE] target_agents={c.target_agents} -> normalized={normalized_agents}\")\n",
    "    # check normalized agents\n",
    "    if \"research_analyst\" in normalized_agents:\n",
    "        return \"research_analyst\"\n",
    "    elif \"hypothesis_generator\" in normalized_agents:\n",
    "        return \"hypothesis_generator\"\n",
    "    elif \"experiment_designer\" in normalized_agents:\n",
    "        return \"experiment_designer\"\n",
    "    print(f\"[ROUTE] fallback to query_type={c.query_type}\")\n",
    "    if c.query_type == \"conceptual\":\n",
    "        return \"research_analyst\"\n",
    "    elif c.query_type == \"design\":\n",
    "        return \"research_analyst\"  # design needs research first\n",
    "    elif c.query_type == \"implementation\":\n",
    "        return \"experiment_designer\"\n",
    "    else:  # planning\n",
    "        return \"research_analyst\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_after_research(state):\n",
    "    c = state.get(\"classification\")\n",
    "    if not c: return \"synth\"\n",
    "    if c.query_type in [\"design\", \"planning\"]: return \"hypothesis_generator\"\n",
    "    return \"synth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_after_hypothesis(state):\n",
    "    c = state.get(\"classification\")\n",
    "    if not c: return \"synth\"\n",
    "    if c.query_type == \"planning\": return \"experiment_designer\"\n",
    "    return \"synth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build graph\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"router\", router_node)\n",
    "workflow.add_node(\"memory_retrieval\", mem_ret_node)\n",
    "workflow.add_node(\"research_analyst\", research_node)\n",
    "workflow.add_node(\"hypothesis_generator\", hypothesis_node)\n",
    "workflow.add_node(\"experiment_designer\", experiment_node)\n",
    "workflow.add_node(\"synth\", synth_node)\n",
    "workflow.add_node(\"memory_update\", mem_upd_node)\n",
    "workflow.set_entry_point(\"router\")\n",
    "workflow.add_conditional_edges(\"router\", route_after_router, {\n",
    "    \"memory_retrieval\": \"memory_retrieval\", \"research_analyst\": \"research_analyst\",\n",
    "    \"hypothesis_generator\": \"hypothesis_generator\", \"experiment_designer\": \"experiment_designer\"\n",
    "})\n",
    "workflow.add_conditional_edges(\"memory_retrieval\", route_after_memory, {\n",
    "    \"research_analyst\": \"research_analyst\", \"hypothesis_generator\": \"hypothesis_generator\",\n",
    "    \"experiment_designer\": \"experiment_designer\"\n",
    "})\n",
    "workflow.add_conditional_edges(\"research_analyst\", route_after_research, {\n",
    "    \"hypothesis_generator\": \"hypothesis_generator\", \"synth\": \"synth\"\n",
    "})\n",
    "workflow.add_conditional_edges(\"hypothesis_generator\", route_after_hypothesis, {\n",
    "    \"experiment_designer\": \"experiment_designer\", \"synth\": \"synth\"\n",
    "})\n",
    "workflow.add_edge(\"experiment_designer\", \"synth\")\n",
    "workflow.add_edge(\"synth\", \"memory_update\")\n",
    "workflow.add_edge(\"memory_update\", END)\n",
    "graph = workflow.compile()\n",
    "print(graph.get_graph().draw_mermaid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(q):\n",
    "    # runs a query through the multi-agent graph\n",
    "    print(f\"\\n{'---'}\\nQUERY: {q}\\n{'---'}\")\n",
    "    try:\n",
    "        s = graph.invoke(create_initial_state(q))\n",
    "        if s is None:\n",
    "            print(\"[ERROR] graph returned None\")\n",
    "            return {}\n",
    "        print(f\"\\n{'---'}\\nRESPONSE:\\n{'---'}\\n{s.get('final_response','')}\\n{'---'}\\nAgents: {set(s.get('agents_activated',[]))}\\n\")\n",
    "        return s\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] graph execution failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exp 1: conceptual\n",
    "r1 = run(\"What are the benefits of multi-agent systems for LLM orchestration vs single-agent approaches?\")\n",
    "if r1:\n",
    "    c1 = r1.get('classification')\n",
    "    print(f\"Analysis: Type={c1.query_type if c1 else 'N/A'}, Papers={r1.get('literature_data',{}).get('papers_found',0) if r1.get('literature_data') else 0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exp 2: design\n",
    "r2 = run(\"Design a hypothesis about improving agent communication in multi-agent LLM systems\")\n",
    "if r2:\n",
    "    h2 = r2.get('hypothesis')\n",
    "    nv2 = r2.get('novelty_score', {})\n",
    "    print(f\"Analysis: Hypothesis={h2.statement[:60] if h2 else 'N/A'}..., Novelty={nv2.get('score','N/A') if nv2 else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exp 3: implementation\n",
    "r3 = run(\"How to implement tool calling with error handling in LangGraph agents?\")\n",
    "if r3:\n",
    "    c3 = r3.get('classification')\n",
    "    print(f\"Analysis: Type={c3.query_type if c3 else 'N/A'}, Agents={set(r3.get('agents_activated',[]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exp 4: planning\n",
    "r4 = run(\"Create a research plan to investigate memory management strategies in multi-agent conversational AI\")\n",
    "if r4:\n",
    "    e4 = r4.get('experiment_plan')\n",
    "    f4 = r4.get('feasibility_score', {})\n",
    "    print(f\"Analysis: Steps={len(e4.steps) if e4 else 0}, Feasibility={f4.get('category','N/A') if f4 else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exp 5: follow-up (tests memory)\n",
    "r5 = run(\"Expand on the previous hypothesis with more TRIZ principles\")\n",
    "if r5:\n",
    "    c5 = r5.get('classification')\n",
    "    print(f\"Analysis: Is followup={c5.is_followup if c5 else 'N/A'}, Memory used={r5.get('memory_context') is not None}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional miscallenous queries\n",
    "queries = [\n",
    "    \"Explain the theoretical trade-offs between Centralized Training with Decentralized Execution (CTDE) and fully decentralized learning in Multi-Agent Reinforcement Learning (MARL).\",\n",
    "\n",
    "    \"Design a fault-tolerant multi-agent architecture for a cross-chain bridge that monitors liquidity pools and validates transactions between Ethereum and Solana.\",\n",
    "\n",
    "    \"How to implement a custom shared experience replay buffer for multiple DQN agents using Ray and Redis in Python?\",\n",
    "\n",
    "    \"Summarize the latest research trends in using Large Language Models for optimizing SQL query execution plans in distributed databases.\",\n",
    "    ]\n",
    "\n",
    "# then run loop\n",
    "for q in queries:\n",
    "    run(q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reflection and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What worked:\n",
    "\n",
    "System works well wiith routing (I had a problem in one of the examples when the Router hallucinated agent names) that resulted in an empty normalization set, the system successfully triggered a fallback mechanism, that resulted in request proceeding, not crashing.\n",
    "\n",
    "Memory management proved effective for follow-up queries, also, in exp 5 expanding on the prev hypothesis pulled chars of context, which allowed agent to refine the specific TRIZ principles generated in exp 2 without need of specific details.\n",
    "\n",
    "Unexpected behaviour and failures:\n",
    "\n",
    "In most of the experiments system invented new agent names which was solved by hardcoding them (mapping) - it is a subject of better prompting (as I guess). Also I think my invented tools for feasibility and novelty calculation as failures cuz they don't provide true intelligence.\n",
    "\n",
    "Areas for growing:\n",
    "\n",
    "Adding a reviwer agent would be perfect and using it before the synthesis step. Also usage of vector memory not just json file would be amazing cuz it would allow the system to retrieve specific segments of past conversations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Routing\n",
    "\n",
    "Router agent correctly classified experiment queries into appropriate types.\n",
    "\n",
    "2) Subjective usefulness\n",
    "\n",
    "Experiments output included real arXiv papers and generated structured hypotheses with novelty scores even though I used simple heuristics for novelty and feasibility calculations.\n",
    "\n",
    "3) Did memory help?\n",
    "\n",
    "Memory was most impactful in Experiment 5 (follow-up query) so I can say, that memory really helped.\n",
    "\n",
    "4) Did tool calls make sense?\n",
    "\n",
    "Yes, tool calls made sense, cause I used arXiv search tool, and again, even though novelty/feasibility calculation tools are synthetic to my taste, I still consider them as useful."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
